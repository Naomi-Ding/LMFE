{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e11c652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[] # import libraries \n",
    "from utils_process import peaks_on_flatten, choose_chunk_peak, calculate_50hz_fourier_coefficient, process_measurement, create_global_features\n",
    "import time \n",
    "import pickle\n",
    "import pyarrow \n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "from train import whole_process_training_single_iter, whole_process_training, whole_Network_training\n",
    "from utils_model import display_metrics\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28a4b22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[] # settings \n",
    "preprocessed = True\n",
    "data_dir = 'vsb-power-line-fault-detection/'\n",
    "recalculate_peaks = False \n",
    "input_dir = 'processed_input/'\n",
    "signal_len = 800000\n",
    "# window_sizes = [2000, 4000, 5000, 8000] \n",
    "window_size = 4000\n",
    "nchunks = int(signal_len / window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6037fdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[] metadata\n",
    "meta_df = pd.read_csv('metadata_train.csv')\n",
    "signal_ids = meta_df['signal_id'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbf9e7d",
   "metadata": {},
   "source": [
    "# STEP 1 Preprocessing & Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e1d72f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ############ STEP 1 Preprocessing & Feature Extraction ######################\n",
    "# =============================================================================\n",
    "if not preprocessed:\n",
    "    signal_df = pq.read_pandas(data_dir + 'train.parquet').to_pandas()\n",
    "    _, all_flat_signals, all_points = peaks_on_flatten(signal_df, signal_ids)\n",
    "\n",
    "    ##### STEP 1A. Denoise & Extract Waveforms #####\n",
    "    # construct waveforms with given window size \n",
    "    waveforms = choose_chunk_peak(all_flat_signals, all_points, window_size=window_size)\n",
    "    print(waveforms.shape)\n",
    "    pickle.dump(waveforms, open(input_dir + 'all_chunk_waves_{}chunks.dat'.format(nchunks), 'wb'))\n",
    "\n",
    "    ##### STEP 1B. Extract Global Features #####\n",
    "    if recalculate_peaks:\n",
    "        signal_fft = calculate_50hz_fourier_coefficient(signal_df.values)\n",
    "        signal_peaks = process_measurement(signal_df, meta_df, signal_fft)\n",
    "        signal_peaks.to_pickle(input_dir + 'signal_peaks.pkl')\n",
    "        del signal_fft\n",
    "        gc.collect()\n",
    "    else:\n",
    "        signal_peaks = pd.read_pickle(input_dir + 'signal_peaks.pkl')\n",
    "    signal_peaks = pd.merge(signal_peaks, meta_df[['signal_id', 'id_measurement', 'target']], on='signal_id', how='left')\n",
    "\n",
    "    ### load KMeans results\n",
    "    # x, x_A, x_B, x_C = pickle.load(open('waves_list.dat','rb'))\n",
    "    # kmeans = KMeans(n_clusters=15, random_state=9, init='k-means++').fit(x)\n",
    "    # kmeans_A = KMeans(n_clusters=6, random_state=9, init='k-means++').fit(x_A)\n",
    "    # kmeans_B = KMeans(n_clusters=6, random_state=9, init='k-means++').fit(x_B)\n",
    "    # kmeans_C = KMeans(n_clusters=6, random_state=9, init='k-means++').fit(x_C)\n",
    "    kmeans = pickle.load(open(input_dir + 'kmeans.dat', 'rb'))\n",
    "    kmeans_A = pickle.load(open(input_dir + 'kmeans_A.dat', 'rb'))\n",
    "    kmeans_B = pickle.load(open(input_dir + 'kmeans_B.dat', 'rb'))\n",
    "    kmeans_C = pickle.load(open(input_dir + 'kmeans_C.dat', 'rb'))\n",
    "\n",
    "    X_global = create_global_features(meta_df, signal_peaks, kmeans, kmeans_A, kmeans_B, kmeans_C)\n",
    "    X_global.to_csv(input_dir + 'global_features.csv')\n",
    "\n",
    "else:\n",
    "    X_global = pd.read_csv(input_dir + 'global_features.csv')\n",
    "    waveforms = pickle.load(open(input_dir + 'all_chunk_waves_{}chunks.dat'.format(nchunks), 'rb'))\n",
    "\n",
    "X_global.set_index('id_measurement', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe3eadb",
   "metadata": {},
   "source": [
    "# STEP 2 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04fede9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# In[] parameters\n",
    "loss_name = 'weighted_bce'\n",
    "output_folder = 'results_{}chunks_{}'.format(nchunks, loss_name)\n",
    "# local_features = True \n",
    "load_local_features = True \n",
    "NN_level = 'signal'\n",
    "NN_model = 'LSTM'\n",
    "Dense_layers = 2\n",
    "NN_pretrained = True \n",
    "layer_idx = 5 \n",
    "NN_batch_size = 512 \n",
    "classifier = 'XGboost'\n",
    "classifier_level = 'measurement'\n",
    "num_folds = 5 \n",
    "num_iterations = 25 \n",
    "feature_set = 'global'\n",
    "kfold_random_state = 123948\n",
    "pretrained = True \n",
    "predict = True \n",
    "weights_dict = None \n",
    "monitor = 'val_loss'\n",
    "dropout = 0.4 \n",
    "regularizer = 'l2'\n",
    "from_logits = True\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cbfdf6",
   "metadata": {},
   "source": [
    "## LMFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ddeacde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:22<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Probability Threshold based on validation set: 0.404\n",
      "MCC Training: 0.946\n",
      "MCC Validation: 0.727\n",
      "MCC Test: 0.730\n",
      "For best probability thresholded: 0.404,\n",
      "         mcc:0.7302598775889093, precision:0.7115716753022453, recall:0.7847619047619048, f1:0.746376811594203, acc:0.967860422405877, roc_auc:0.8821818562529445,average_precision:0.5713849585281522\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chao Huang\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    }
   ],
   "source": [
    "########### LMFE ########### \n",
    "_, best_proba_LMFE, metrics_LMFE, test_pred_LMFE = whole_process_training(meta_df, waveforms, X_global,\n",
    "    local_features=True, NN_level=NN_level, NN_model=NN_model,\n",
    "    Dense_layers=Dense_layers, NN_pretrained=NN_pretrained, layer_idx=layer_idx, NN_batch_size=NN_batch_size, \n",
    "    output_folder=output_folder, classifier=classifier, classifier_level=classifier_level, num_folds=num_folds,\n",
    "    num_iterations=num_iterations, feature_set=feature_set, kfold_random_state=kfold_random_state, \n",
    "    load_local_features=load_local_features, pretrained=pretrained, predict=predict, early_stopping_rounds=100, \n",
    "    verbose_eval=0, weights_dict=weights_dict, monitor=monitor, dropout=dropout, regularizer=regularizer, \n",
    "    loss_name=loss_name, from_logits=from_logits, n_epochs=n_epochs)\n",
    "# test_pred_LMFE.to_csv(output_folder + '/test_pred_LMFE.csv')\n",
    "metrics_LMFE = display_metrics(test_pred_LMFE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4123e67",
   "metadata": {},
   "source": [
    "## Only global-scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76d3935f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########### Only global-scale features ########### \n",
    "###### all global-scale features\n",
    "# _, best_proba_global, metrics_global, test_pred_global = whole_process_training(meta_df, waveforms, X_global,\n",
    "#     local_features=False, NN_level=NN_level, NN_model=NN_model, \n",
    "#     Dense_layers=Dense_layers, NN_pretrained=NN_pretrained, layer_idx=layer_idx, NN_batch_size=NN_batch_size, \n",
    "#     output_folder=output_folder, classifier='LightGBM', classifier_level=classifier_level, num_folds=num_folds,\n",
    "#     num_iterations=num_iterations, feature_set='global', kfold_random_state=kfold_random_state, \n",
    "#     load_local_features=load_local_features, pretrained=pretrained, predict=predict, early_stopping_rounds=100, \n",
    "#     verbose_eval=0, weights_dict=weights_dict, monitor=monitor, dropout=dropout, regularizer=regularizer, \n",
    "#     loss_name=loss_name, from_logits=from_logits, n_epochs=n_epochs)\n",
    "# test_pred_global.to_csv(output_folder + '/test_pred_global.csv')\n",
    "test_pred_global = pd.read_csv(output_folder + '/test_pred_global.csv')\n",
    "metrics_global = display_metrics(test_pred_global)\n",
    "\n",
    "###### only phase-level features\n",
    "# _, best_proba_phase, metrics_phase, test_pred_phase = whole_process_training(meta_df, waveforms, X_global,\n",
    "#     local_features=False, NN_level=NN_level, NN_model=NN_model, \n",
    "#     Dense_layers=Dense_layers, NN_pretrained=NN_pretrained, layer_idx=layer_idx, NN_batch_size=NN_batch_size, \n",
    "#     output_folder=output_folder, classifier='LightGBM', classifier_level=classifier_level, num_folds=num_folds,\n",
    "#     num_iterations=num_iterations, feature_set='phase_level', kfold_random_state=kfold_random_state, \n",
    "#     load_local_features=load_local_features, pretrained=pretrained, predict=predict, early_stopping_rounds=100, \n",
    "#     verbose_eval=0, weights_dict=weights_dict, monitor=monitor, dropout=dropout, regularizer=regularizer, \n",
    "#     loss_name=loss_name, from_logits=from_logits, n_epochs=n_epochs)\n",
    "# test_pred_phase.to_csv(output_folder + '/test_pred_phase.csv')\n",
    "test_pred_phase = pd.read_csv(output_folder + '/test_pred_phase.csv')\n",
    "metrics_phase = display_metrics(test_pred_phase)\n",
    "\n",
    "###### only measurement-level features\n",
    "# _, best_proba_measure, metrics_measure, test_pred_measure = whole_process_training(meta_df, waveforms, X_global,\n",
    "#     local_features=False, NN_level=NN_level, NN_model=NN_model, \n",
    "#     Dense_layers=Dense_layers, NN_pretrained=NN_pretrained, layer_idx=layer_idx, NN_batch_size=NN_batch_size, \n",
    "#     output_folder=output_folder, classifier='LightGBM', classifier_level=classifier_level, num_folds=num_folds,\n",
    "#     num_iterations=num_iterations, feature_set='measurement_level', kfold_random_state=kfold_random_state, \n",
    "#     load_local_features=load_local_features, pretrained=pretrained, predict=predict, early_stopping_rounds=100, \n",
    "#     verbose_eval=0, weights_dict=weights_dict, monitor=monitor, dropout=dropout, regularizer=regularizer, \n",
    "#     loss_name=loss_name, from_logits=from_logits, n_epochs=n_epochs)\n",
    "# test_pred_measure.to_csv(output_folder + '/test_pred_measure.csv')\n",
    "test_pred_measure = pd.read_csv(output_folder + '/test_pred_measure.csv')\n",
    "metrics_measure = display_metrics(test_pred_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2347de91",
   "metadata": {},
   "source": [
    "## Only local-scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e17b3896",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########### Only local-scale features ########### \n",
    "#### Prediction on test set\n",
    "# _, best_proba_RNN, metrics_RNN, test_pred_RNN, _ = whole_Network_training(meta_df, waveforms,\n",
    "#     NN_level=NN_level, NN_model=NN_model, Dense_layers=Dense_layers, NN_pretrained=NN_pretrained, \n",
    "#     layer_idx=layer_idx, NN_batch_size=NN_batch_size, indice_level=classifier_level,\n",
    "#     output_folder=output_folder, kfold_random_state=kfold_random_state, num_folds=num_folds,\n",
    "#     num_iterations=num_iterations, predict=predict, monitor=monitor, dropout=dropout, regularizer=regularizer,\n",
    "#     from_logits=from_logits, loss_name=loss_name, extract_attention_weights=False)\n",
    "# test_pred_RNN.to_csv(output_folder + '/test_pred_RNN.csv')\n",
    "# load test set prediction\n",
    "test_pred_RNN = pd.read_csv(output_folder + '/test_pred_RNN.csv')\n",
    "metrics_RNN = display_metrics(test_pred_RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9046062b",
   "metadata": {},
   "source": [
    "# Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2142966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   MCC  Precision    Recall  F1 Score       AUC\n",
      "LMFE          0.730260   0.711572  0.784762  0.746377  0.882182\n",
      "Global-scale  0.705548   0.735700  0.710476  0.722868  0.847054\n",
      "Phase         0.708669   0.712454  0.740952  0.726424  0.860888\n",
      "Measurement   0.706481   0.796296  0.655238  0.718913  0.822245\n",
      "RNN           0.679368   0.632970  0.775238  0.696918  0.873206\n"
     ]
    }
   ],
   "source": [
    "# In[] display the performance\n",
    "all_metrics = np.array([metrics_LMFE, metrics_global, metrics_phase, metrics_measure, metrics_RNN])\n",
    "df_res = pd.DataFrame(data=all_metrics, index=['LMFE', 'Global-scale', 'Phase', 'Measurement', 'RNN'], \n",
    "    columns=['MCC', 'Precision', 'Recall', 'F1 Score', 'AUC'])\n",
    "print(df_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
