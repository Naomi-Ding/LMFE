{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edceabe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[] # import libraries \n",
    "from utils_process import peaks_on_flatten, choose_chunk_peak, calculate_50hz_fourier_coefficient, process_measurement, create_global_features\n",
    "import time \n",
    "import pickle\n",
    "import pyarrow \n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "from train import whole_process_training_single_iter, whole_process_training, whole_Network_training\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07580c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[] # settings \n",
    "preprocessed = True\n",
    "recalculate_peaks = False \n",
    "data_dir = 'processed_input/'\n",
    "signal_len = 800000\n",
    "# window_sizes = [2000, 4000, 5000, 8000] \n",
    "window_size = 4000\n",
    "nchunks = int(signal_len / window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c394b4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[] parameters\n",
    "loss_name = 'weighted_bce'\n",
    "output_folder = 'results_{}chunks_{}'.format(nchunks, loss_name)\n",
    "local_features = True \n",
    "load_local_features = True \n",
    "NN_level = 'signal'\n",
    "NN_model = 'LSTM'\n",
    "Dense_layers = 2\n",
    "NN_pretrained = True \n",
    "layer_idx = 5 \n",
    "NN_batch_size = 512 \n",
    "classifier = 'XGboost'\n",
    "classifier_level = 'measurement'\n",
    "num_folds = 5 \n",
    "num_iterations = 25 \n",
    "feature_set = 'global'\n",
    "kfold_random_state = 123948\n",
    "pretrained = True \n",
    "predict = True \n",
    "weights_dict = None \n",
    "monitor = 'val_loss'\n",
    "dropout = 0.4 \n",
    "regularizer = 'l2'\n",
    "from_logits = True\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e6e70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[] metadata\n",
    "meta_df = pd.read_csv('metadata_train.csv')\n",
    "signal_ids = meta_df['signal_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24eae4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ############ STEP 1 Preprocessing & Feature Extraction ######################\n",
    "# =============================================================================\n",
    "if not preprocessed:\n",
    "    signal_df = pq.read_pandas(data_dir + 'train.parquet').to_pandas()\n",
    "    _, all_flat_signals, all_points = peaks_on_flatten(signal_df, signal_ids)\n",
    "\n",
    "    ##### STEP 1A. Denoise & Extract Waveforms #####\n",
    "    # construct waveforms with given window size \n",
    "    waveforms = choose_chunk_peak(all_flat_signals, all_points, window_size=window_size)\n",
    "    print(waveforms.shape)\n",
    "    pickle.dump(waveforms, open(data_dir + 'all_chunk_waves_{}chunks.dat'.format(nchunks), 'wb'))\n",
    "\n",
    "    ##### STEP 1B. Extract Global Features #####\n",
    "    if recalculate_peaks:\n",
    "        signal_fft = calculate_50hz_fourier_coefficient(signal_df.values)\n",
    "        signal_peaks = process_measurement(signal_df, meta_df, signal_fft)\n",
    "        signal_peaks.to_pickle('signal_peaks.pkl')\n",
    "        del signal_fft\n",
    "        gc.collect()\n",
    "    else:\n",
    "        signal_peaks = pd.read_pickle(data_dir + 'signal_peaks.pkl')\n",
    "    signal_peaks = pd.merge(signal_peaks, meta_df[['signal_id', 'id_measurement', 'target']], on='signal_id', how='left')\n",
    "\n",
    "    ### load KMeans results\n",
    "    # x, x_A, x_B, x_C = pickle.load(open('waves_list.dat','rb'))\n",
    "    # kmeans = KMeans(n_clusters=15, random_state=9, init='k-means++').fit(x)\n",
    "    # kmeans_A = KMeans(n_clusters=6, random_state=9, init='k-means++').fit(x_A)\n",
    "    # kmeans_B = KMeans(n_clusters=6, random_state=9, init='k-means++').fit(x_B)\n",
    "    # kmeans_C = KMeans(n_clusters=6, random_state=9, init='k-means++').fit(x_C)\n",
    "    kmeans = pickle.load(open(data_dir + 'kmeans.dat', 'rb'))\n",
    "    kmeans_A = pickle.load(open(data_dir + 'kmeans_A.dat', 'rb'))\n",
    "    kmeans_B = pickle.load(open(data_dir + 'kmeans_B.dat', 'rb'))\n",
    "    kmeans_C = pickle.load(open(data_dir + 'kmeans_C.dat', 'rb'))\n",
    "\n",
    "    X_global = create_global_features(meta_df, signal_peaks, kmeans, kmeans_A, kmeans_B, kmeans_C)\n",
    "    X_global.to_csv(data_dir + 'global_features.csv')\n",
    "\n",
    "else:\n",
    "    X_global = pd.read_csv(data_dir + 'global_features.csv')\n",
    "    if not load_local_features:\n",
    "        waveforms = pickle.load(open(data_dir + 'all_chunk_waves_{}chunks.dat'.format(nchunks), 'rb'))\n",
    "    else:\n",
    "        waveforms = None \n",
    "\n",
    "X_global.set_index('id_measurement', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20cc79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ###################### STEP 2 Model Training ################################\n",
    "# =============================================================================\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "_, best_proba, metrics, test_pred = whole_process_training(meta_df, waveforms, X_global,\n",
    "        local_features=local_features, NN_level=NN_level, NN_model=NN_model,\n",
    "        Dense_layers=Dense_layers, NN_pretrained=NN_pretrained, layer_idx=layer_idx, NN_batch_size=NN_batch_size, \n",
    "        output_folder=output_folder, classifier=classifier, classifier_level=classifier_level, num_folds=num_folds,\n",
    "        num_iterations=num_iterations, feature_set=feature_set, kfold_random_state=kfold_random_state, \n",
    "        load_local_features=load_local_features, pretrained=pretrained, predict=predict, early_stopping_rounds=100, \n",
    "        verbose_eval=0, weights_dict=weights_dict, monitor=monitor, dropout=dropout, regularizer=regularizer, \n",
    "        loss_name=loss_name, from_logits=from_logits, n_epochs=n_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42bed88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
