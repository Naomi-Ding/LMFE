{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a97075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[] # import libraries \n",
    "from utils_process import peaks_on_flatten, choose_chunk_peak, calculate_50hz_fourier_coefficient, process_measurement, create_global_features\n",
    "import time \n",
    "import pickle\n",
    "import pyarrow \n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "from train import whole_process_training_single_iter, whole_process_training, whole_Network_training\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8b51951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[] # settings \n",
    "preprocessed = True\n",
    "recalculate_peaks = False \n",
    "data_dir = 'processed_input/'\n",
    "signal_len = 800000\n",
    "# window_sizes = [2000, 4000, 5000, 8000] \n",
    "window_size = 4000\n",
    "nchunks = int(signal_len / window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ea63bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[] parameters\n",
    "loss_name = 'weighted_bce'\n",
    "output_folder = 'results_{}chunks_{}'.format(nchunks, loss_name)\n",
    "local_features = True \n",
    "load_local_features = True \n",
    "NN_level = 'signal'\n",
    "NN_model = 'LSTM'\n",
    "Dense_layers = 2\n",
    "NN_pretrained = True \n",
    "layer_idx = 5 \n",
    "NN_batch_size = 512 \n",
    "classifier = 'XGboost'\n",
    "classifier_level = 'measurement'\n",
    "num_folds = 5 \n",
    "num_iterations = 25 \n",
    "feature_set = 'global'\n",
    "kfold_random_state = 123948\n",
    "pretrained = True \n",
    "predict = True \n",
    "weights_dict = None \n",
    "monitor = 'val_loss'\n",
    "dropout = 0.4 \n",
    "regularizer = 'l2'\n",
    "from_logits = True\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99d7c8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[] metadata\n",
    "meta_df = pd.read_csv('metadata_train.csv')\n",
    "signal_ids = meta_df['signal_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b458b03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ############ STEP 1 Preprocessing & Feature Extraction ######################\n",
    "# =============================================================================\n",
    "if not preprocessed:\n",
    "    signal_df = pq.read_pandas(data_dir + 'train.parquet').to_pandas()\n",
    "    _, all_flat_signals, all_points = peaks_on_flatten(signal_df, signal_ids)\n",
    "\n",
    "    ##### STEP 1A. Denoise & Extract Waveforms #####\n",
    "    # construct waveforms with given window size \n",
    "    waveforms = choose_chunk_peak(all_flat_signals, all_points, window_size=window_size)\n",
    "    print(waveforms.shape)\n",
    "    pickle.dump(waveforms, open(data_dir + 'all_chunk_waves_{}chunks.dat'.format(nchunks), 'wb'))\n",
    "\n",
    "    ##### STEP 1B. Extract Global Features #####\n",
    "    if recalculate_peaks:\n",
    "        signal_fft = calculate_50hz_fourier_coefficient(signal_df.values)\n",
    "        signal_peaks = process_measurement(signal_df, meta_df, signal_fft)\n",
    "        signal_peaks.to_pickle('signal_peaks.pkl')\n",
    "        del signal_fft\n",
    "        gc.collect()\n",
    "    else:\n",
    "        signal_peaks = pd.read_pickle(data_dir + 'signal_peaks.pkl')\n",
    "    signal_peaks = pd.merge(signal_peaks, meta_df[['signal_id', 'id_measurement', 'target']], on='signal_id', how='left')\n",
    "\n",
    "    ### load KMeans results\n",
    "    # x, x_A, x_B, x_C = pickle.load(open('waves_list.dat','rb'))\n",
    "    # kmeans = KMeans(n_clusters=15, random_state=9, init='k-means++').fit(x)\n",
    "    # kmeans_A = KMeans(n_clusters=6, random_state=9, init='k-means++').fit(x_A)\n",
    "    # kmeans_B = KMeans(n_clusters=6, random_state=9, init='k-means++').fit(x_B)\n",
    "    # kmeans_C = KMeans(n_clusters=6, random_state=9, init='k-means++').fit(x_C)\n",
    "    kmeans = pickle.load(open(data_dir + 'kmeans.dat', 'rb'))\n",
    "    kmeans_A = pickle.load(open(data_dir + 'kmeans_A.dat', 'rb'))\n",
    "    kmeans_B = pickle.load(open(data_dir + 'kmeans_B.dat', 'rb'))\n",
    "    kmeans_C = pickle.load(open(data_dir + 'kmeans_C.dat', 'rb'))\n",
    "\n",
    "    X_global = create_global_features(meta_df, signal_peaks, kmeans, kmeans_A, kmeans_B, kmeans_C)\n",
    "    X_global.to_csv(data_dir + 'global_features.csv')\n",
    "\n",
    "else:\n",
    "    X_global = pd.read_csv(data_dir + 'global_features.csv')\n",
    "    if not load_local_features:\n",
    "        waveforms = pickle.load(open(data_dir + 'all_chunk_waves_{}chunks.dat'.format(nchunks), 'rb'))\n",
    "    else:\n",
    "        waveforms = None \n",
    "\n",
    "X_global.set_index('id_measurement', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "845d066e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:22<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Probability Threshold based on validation set: 0.404\n",
      "MCC Training: 0.946\n",
      "MCC Validation: 0.727\n",
      "MCC Test: 0.730\n",
      "For best probability thresholded: 0.404,\n",
      "         mcc:0.7302598775889093, precision:0.7115716753022453, recall:0.7847619047619048, f1:0.746376811594203, acc:0.967860422405877, roc_auc:0.8821818562529445,average_precision:0.5713849585281522\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chao Huang\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:870: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ###################### STEP 2 Model Training ################################\n",
    "# =============================================================================\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "_, best_proba, metrics, test_pred = whole_process_training(meta_df, waveforms, X_global,\n",
    "        local_features=local_features, NN_level=NN_level, NN_model=NN_model,\n",
    "        Dense_layers=Dense_layers, NN_pretrained=NN_pretrained, layer_idx=layer_idx, NN_batch_size=NN_batch_size, \n",
    "        output_folder=output_folder, classifier=classifier, classifier_level=classifier_level, num_folds=num_folds,\n",
    "        num_iterations=num_iterations, feature_set=feature_set, kfold_random_state=kfold_random_state, \n",
    "        load_local_features=load_local_features, pretrained=pretrained, predict=predict, early_stopping_rounds=100, \n",
    "        verbose_eval=0, weights_dict=weights_dict, monitor=monitor, dropout=dropout, regularizer=regularizer, \n",
    "        loss_name=loss_name, from_logits=from_logits, n_epochs=n_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d51afdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
